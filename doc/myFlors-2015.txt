Implementing FLORS, Schnabel & SchÃ¼tze, TACL, 2014

Corpus used by FLORS:

- training: PTB-3, sec. 2-21 + PTB 1988 WSJ 100.000 unlabeled
	/Users/gune00/data/MLDP/english (conll format of sec. 2-21 of PTB version 3 + extracted sentences)
		- this should give 39,832 sentences -> I have 39,278
		- I think it is, because I am ussing CONLL PTB and comments MLDP/2009/conll09-english/
			It matches sections 2 through 21 of Treebank. Note that a few sentences from the original 
  			Treebank were removed due to merging problems between the input corpora.
	/Users/gune00/data/BioNLPdata/CoNLL2007/ptb/unlab (tokenized/sentence/line) 
		-> only read first 100.000
	
- test domain 
	/Volumes/data1/sancl-2012/sancl.all -> labeled and unlabeled data (conll format and tokenized/sentence/line) 
		-> it is said that unlabeled data has 100T sentences each which is not true
	/Users/gune00/data/BioNLPdata/CoNLL2007/pbiotb (conll format and tokenized/sentence/line) 
		-> only read first 100.000

Idea is to train a one-vs-all POS-tagger and NE-tagger.

One-vs-all means: 
- for each tag train a model. Then, apply all models on new word and select best tag.
- Should be part of the Liblinear package.

Background from paper:

Local context for tagging word:
	window 2l+1 around token v_i: (v_i-l, .., v_i, ..., v_i+l)

	sentences are padded with <BOUNDARY> at the start/end
	
	if l = 2 -> 5-token window (v_i-2, v_i-1, v_i, v_i+1, v_i+2)

Representation of feature F for token v_i

	F(v_i) = f(v_i-l) conc ... conc f(v_i+l)
	
	
Word features: each word w is represented by four components
(normalized by unit vector):

f(w) = f_left(w) + f_right(w) + f_suffix(x) + f_shape(w)

1. Distributed word feature:

f_left(w) ::=
	cell i gets value x_i = tf(freq(bigram(c_i, w)))
	"the ith entry named x_i is the weighted number of times 
		that the INDICATOR WORD c_i occurs immediately to the left of w"
	
	where c_i is the word with frequency rank i in the corpus.
	
f_right(w) analoguously.

restrict n=500 indicator words.

To avoid zero vectors: add additional cell n+1 and count omitted context.

text is preprocessed by lowercasing everything.

2. Suffix features: all suffixes of a word plus word:
	they compute 91.161 different suffix features from training data I guess
	
3. Shape features:
	Use Berkely parser word features.
	Each word is mapped to 16 bit string indicating ENGLISH orthography
	on WSJ: yields 50 different signatures (bit string instances).
	This gives 50 different shape features.

-> https://github.com/slavpetrov/berkeleyparser/blob/master/src/edu/berkeley/nlp/discPCFG/LexiconFeatureExtractor.java
-> https://github.com/slavpetrov/berkeleyparser/blob/master/src/edu/berkeley/nlp/discPCFG/LexiconFeature.java
	
STATUS
	
1. Computation of distributional vectors: -> NOTE it is ako word2vec version
	-> DONE, cf. /Users/gune00/data/wordVectorTests, indicator words: resources/iw.txt
	-> NOTE: us any token as basis for indictaor word (e.g., also numbers)

2. Compute suffix features
	-> DONE
	-> only use words (no numbers of form digit.*digit|digit
	-> i compute 91144 suffixes (instead of 91,161 as in FLORS)
	
3.	Shape Features:
	-> DONE
	-> use all tokens
	-> I compute 49 signatures as instead of 50
	
4. Now, I that I can create all features, I could start doing learning

- it is still unclear how the unlabeled data is used in training phase
- so far I understand: all data is used to create distributed word vectors, i.e., source domain knowledge and the test domain knowledge
-> so following Blitzer et al (2006): this should merge hidden distributional information from source domain and test domain
-> this is the only place, where I see how unlabeled training data is used
-> THIS MEANS: one could also do testing with fewer test domain knowledge ...

BEFORE starting, I will define a similarity measure for twoo words based on the distributional features, and will check
what it means -> similar equations ...



2. Training of POS tagger

- use conll dependency treebanks or SVM format  ->
	each line corresponds to a sequence of tokens, which are eventually labeled
	w1/t1, w2/t2, ..., wn/tn


- the idea is in principle very simple - iterate through sentence and create per token:
	training: define a feature vector of
		label feat:num ... feat:num
	testing/predicting
		feat:num ... feat:num
- hence the major steps are the encoding/decoding 
	of the training/input data 

- following MDP, I would need classes for:
	- Alphabet: string2num and num2string mappings -> DONE via corpus.DistributedWordVectorFactory
		- for all labels
		- for all words
		
	- FeatureExtractor -> a class that keeps the feature templates
		for computing suffixes -> DONE
		for computing shape features -> DONE
	
- initialize: -> DONE
	read in word vectors and indicatorWords
	initialize bijective indexes
	
- read in training corpus line wise (assuming each line corresponds to an training/testing instance)

- read in training file:
	- for each sentence
		- create word data element:
			- word
			- label
			- word vector left
			- word vector right
			- compute suffixes
			- compute shape
			
		- NOTE: keep the indexes so that symbolic input/output can be reconstructed, cf. http://deeplearning.net/tutorial/rnnslu.html