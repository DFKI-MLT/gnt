QUESTIONS CONCERNING UNLABELED DATA:

THINK: why does clustering help?
	- clustering computes a classification of words into classes
	- thus, if I have an unknown word, but it is clustered into the same class as a word that was known during training
	- then the learner can just use the learned features
	
	- but if a word belongs to a class that was not observed in training, it will use this class as feature value any way
	- BUT what does this mean for the learner and the learned model?
	-> in principle this should also be true when using POS classes as features, 
		because it can happen that a word in the test corpus has a POS class that was not observed in the training phase

THINK: about distributed vector representations
	- currently, they are used for representing words
	- the vectors are build on basis of indicator words, which define the dimension, i.e., the vector space
	
	- I think the same behavior as above also is correct here:
		if a word has a distributed vector which was not used during labeled training, 
		the tagger will use it to fill the feature vector in any case
		
SO WHAT does this mean? 
- is it correct to use the "unknown" feature values?

if not: then predict empty feature? (as I do for shape and suffix features)
if learn a feature for the UNKNOWN class, by randomly change known POS tags to UNKNOWN during training?

I am wondering:
Do I currently use the cluster ids of OOD words wrongly ? Or will liblinear take care about this?
By computing basically a distance and a method which says: it is near enough this class that I know?
Which is controlled by the error rate?
