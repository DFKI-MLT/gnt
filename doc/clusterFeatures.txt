October, 2015:

- integration of clusters from /Users/gune00/data/Marmot/Word

	- use preprocessed cluster first:

	- use cluster name as feature

	- create word2clusterID  map

	- unknown words are mapped to clusterID of word <RARE>|<Rare>

-> DONE

- how should I represent word-clusterID ?

-> as binary ala shape feature reusing code of shape

-> since Marmot uses digit normalization, 
	I also use when accessing liblinear cluster id with word w

-> First tests: worked! and shows improvement for NER of about 4,8% F1

Try them with POS tagger as well -> OK, brings improvements

*************
Feb, 2016:

About unknown words:

- All words not member of the clusterId-Map are mapped to dummy word <RARE>.
- Words in the clusterId-Map are NOT lower-cased; only normalization is done by mapping digits to 0

- Currently, no tokenization is performed, because implicitly used via the labeled treebanks

- If I have free text, then tokenization must be done with tokenizer that is also used for creating word clusters
- OR tokenization-inconsistent words will be treated as <RARE>

- SPELLING variation also can not be handled, because we perform exact match with clusterId-Map
	- could be realized by NemexA but then what are useful sim-fct and sim-threshold?


- Cluster from Jon Dehardy - clusterCat
	- run on source text on clusterCat and Marlin
	- use same parameters
		- number of cluster, frequency threshold, iterations
	- difference between clusterCat and Marlin:
		clusterCat uses "leftWord word rightWord bigrams" (similar to word clusters in GNT)
		Marlin uses "leftWord leftWord word" trigrams
	- Jon proposed to use clusters and cluster size as additional feature

*************
September, 2016:

Using clusters computed by Marlin:

- /Users/gune00/dfki/POS_MORPH_NER_TAGGING/cistern/cistern-master/marlin

- test and works

- corpus must be preprocessed:
	- each line a sentence
	- tokenized
	- numbers are digitized

- Preprocessing used by Marlin for English:
	- implementation of PUNKT for sentence recognition
	- Stanford tokenizer for English
	
- I have them preprocessed and tokenized by Morphadorner
	- unclear how good/bad
	
How can I use phrasal clusters?
