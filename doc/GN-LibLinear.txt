GN on 6th December 2015

Training and application with Java library LibLinear: 
http://liblinear.bwaldvogel.de/ which is based on C++ LibLinear
http://www.csie.ntu.edu.tw/~cjlin/liblinear/

*******************************************************************************
General usage of API (from http://liblinear.bwaldvogel.de/ & https://github.com/bwaldvogel/liblinear-java):

	Problem problem = new Problem();
	problem.l = ... // number of training examples
	problem.n = ... // number of features
	problem.x = ... // feature nodes
	problem.y = ... // target values

	SolverType solver = SolverType.L2R_LR; // -s 0
	double C = 1.0;    // cost of constraints violation
	double eps = 0.01; // stopping criteria; influences number of iterations performed, the higher the less

	Parameter parameter = new Parameter(solver, C, eps);
	Model model = Linear.train(problem, parameter);
	File modelFile = new File("model");
	model.save(modelFile);
	// load model or use it directly
	model = Model.load(modelFile);

	Feature[] instance = { new FeatureNode(1, 4), new FeatureNode(2, 2) };
	double prediction = Linear.predict(model, instance);

About training data input format for Liblinear:
- 	each line is actually an integer representation of a feature vector instance occurrence, e.g.,

-	1 8:1 9:1 10:1 11:1 ...  134:1 135:1 136:1
	- First element is a class label (1=N, 2=Y, in case of sentence splitter), and then N feature-value-index:value
	- Feature-value-index is just the individual index of a concrete instance
	- value is a real number or integer feature-value-index relevance; 
	- if only binary features are used (which is the case in MDP), then 1 indicates present feature.
		All non-present features (non-observed) have value 0.
		
Note: the lines encode token level, not type level ! This is the basis for the counting in the learner.

Thus a major step is to define the classes and feature vectors, and to create an integer representation.

For this, an alphabet has to be defined that keeps all these encodings.

This MDP-based Liblinear interface also supports parallel processing of split models.
So, if I can work out this interface, adaptation to other learning tasks should be easy possible.

*******************************************************************************

Using LibLinear in GNT:

- currently I have real-valued features and binary features for each word
- I concatenate them all into one large feature vector
- and then concat all features of all words in window into one large vector
- to do so I compute an enumeration

- I do not use:
	- bias term
	- scaling
	
- if bias is set to value b > 0, then extra feature with value b is added to each problem instance
- I guess it is done automatically -> bias value b is in model file ... but I do not see any effect.

- But because I do not do scaling, does it mean that binary features are most important?
	- I used scaling in trainer.ProblemInstance.createProblemInstanceFromWindow(Window) 
		-> instead of 1 I used 1/length-of-feature-set
	-> not successfully -> BUT: Do I do it correctly ?
	
-> NOTE: scaling must be done for train and test data:
	-> http://stackoverflow.com/questions/10055396/scaling-the-testing-data-for-libsvm-matlab-implementation

