22.9.2015, GN:

Note tagging with SVM-NER:
*******************************************************************************

tagging Conll 2003 types according to data define in

/Users/gune00/dfki/TIM-Tools/SvenTagger/NERtests/conll2003

models: trained on eng/deu.train.svm 
EN2003 -  testb F1=79.90%
DE2003 -  testb F1=61.37

models: trained on eng/deu.traintesta.svm 
EN2003 -  testb F1=80.56% (minus 11%)
DE2003 -  testb F1=66.58  (minus 11%)

Testing NER:
-> had to update corpus data as well!

eng.train -> mapped to conll format eng.train.conll via:
mapper.transcodeNERfile("resources/data/ner/eng.testb", "utf-8", "resources/data/ner/eng-testb.conll", "utf-8");

// I have adjusted the NER conll format to be consistent with the other conll formats, i.e.
// LONDON NNP I-NP I-LOC -> 1	LONDON	NNP	I-NP	I-LOC
// This is why I have 5 elements instead of 4 


Using cluster IDs:
*******************


./conlleval < eval/eng-testa.txt 
processed 51578 tokens with 5942 phrases; found: 6078 phrases; correct: 5319.
accuracy:  98.24%; precision:  87.51%; recall:  89.52%; FB1:  88.50
              LOC: precision:  91.91%; recall:  92.81%; FB1:  92.36
             MISC: precision:  83.98%; recall:  81.89%; FB1:  82.92
              ORG: precision:  77.92%; recall:  82.10%; FB1:  79.96
              PER: precision:  91.99%; recall:  95.44%; FB1:  93.69

./conlleval < eval/eng-testb.txt 
processed 46666 tokens with 5648 phrases; found: 5919 phrases; correct: 4787.
accuracy:  97.16%; precision:  80.88%; recall:  84.76%; FB1:  82.77
              LOC: precision:  85.01%; recall:  88.73%; FB1:  86.83
             MISC: precision:  71.66%; recall:  74.93%; FB1:  73.26
              ORG: precision:  73.12%; recall:  79.41%; FB1:  76.13
              PER: precision:  89.15%; recall:  90.41%; FB1:  89.78


This is close to CRF  -> but without POS and Chunks 
-> still missing about 9% FB1 to the best NER for English


lt-pool-183:resources gune00$ ./conlleval < eval/eng-testa.txt 
processed 51578 tokens with 5942 phrases; found: 6093 phrases; correct: 5280.
accuracy:  98.13%; precision:  86.66%; recall:  88.86%; FB1:  87.74
              LOC: precision:  91.85%; recall:  93.20%; FB1:  92.52
             MISC: precision:  82.63%; recall:  81.02%; FB1:  81.82
              ORG: precision:  77.25%; recall:  80.76%; FB1:  78.96
              PER: precision:  90.38%; recall:  94.35%; FB1:  92.32
lt-pool-183:resources gune00$ ./conlleval < eval/eng-testb.txt 
processed 46666 tokens with 5648 phrases; found: 5925 phrases; correct: 4744.
accuracy:  96.95%; precision:  80.07%; recall:  83.99%; FB1:  81.98
              LOC: precision:  85.12%; recall:  88.85%; FB1:  86.95
             MISC: precision:  72.40%; recall:  74.36%; FB1:  73.37
              ORG: precision:  71.61%; recall:  77.90%; FB1:  74.63
              PER: precision:  87.32%; recall:  89.42%; FB1:  88.36
              
-> Now, the differences with previous tests is that I now use all sentences from unlabeded data, 
	not only the first 1000
-> SO, maybe run without external unlabeled file as for German
-> HERE: only using unlabeled sentences from train/test/ etc.

lt-pool-183:resources gune00$ ./conlleval < eval/eng-testa.txt 
processed 51578 tokens with 5942 phrases; found: 6061 phrases; correct: 5301.
accuracy:  98.25%; precision:  87.46%; recall:  89.21%; FB1:  88.33
              LOC: precision:  91.97%; recall:  92.32%; FB1:  92.15
             MISC: precision:  84.54%; recall:  82.43%; FB1:  83.47
              ORG: precision:  77.39%; recall:  81.95%; FB1:  79.61
              PER: precision:  91.99%; recall:  94.79%; FB1:  93.37
lt-pool-183:resources gune00$ ./conlleval < eval/eng-testb.txt 
processed 46666 tokens with 5648 phrases; found: 5917 phrases; correct: 4750.
accuracy:  97.04%; precision:  80.28%; recall:  84.10%; FB1:  82.14
              LOC: precision:  84.66%; recall:  87.35%; FB1:  85.98
             MISC: precision:  71.70%; recall:  75.78%; FB1:  73.68
              ORG: precision:  72.29%; recall:  78.21%; FB1:  75.13
              PER: precision:  88.23%; recall:  90.41%; FB1:  89.31

Using suffix of lenght <= 5

lt-pool-183:resources gune00$ ./conlleval < eval/eng-testa.txt 
processed 51578 tokens with 5942 phrases; found: 6048 phrases; correct: 5343.
accuracy:  98.35%; precision:  88.34%; recall:  89.92%; FB1:  89.12
              LOC: precision:  92.40%; recall:  93.30%; FB1:  92.85
             MISC: precision:  84.62%; recall:  83.51%; FB1:  84.06
              ORG: precision:  80.26%; recall:  82.77%; FB1:  81.50
              PER: precision:  92.05%; recall:  94.95%; FB1:  93.48
lt-pool-183:resources gune00$ ./conlleval < eval/eng-testb.txt 
processed 46666 tokens with 5648 phrases; found: 5912 phrases; correct: 4788.
accuracy:  97.18%; precision:  80.99%; recall:  84.77%; FB1:  82.84
              LOC: precision:  84.85%; recall:  88.61%; FB1:  86.69
             MISC: precision:  71.01%; recall:  76.07%; FB1:  73.45
              ORG: precision:  74.31%; recall:  78.87%; FB1:  76.52
              PER: precision:  88.58%; recall:  90.66%; FB1:  89.61