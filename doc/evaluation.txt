check tests according to accuracy etc.

eventually liblinear comes with own software

else use existing conll based packages
- I will the conlleval scripts -> copy in /Users/gune00/data/wordVectorTests/bin
- Usefull also for NER

- about conlleval script

#!/usr/bin/perl -w
# conlleval: evaluate result of processing CoNLL-2000 shared task
# usage:     conlleval [-l] [-r] [-d delimiterTag] [-o oTag] < file
#            README: http://cnts.uia.ac.be/conll2000/chunking/output.html
# options:   l: generate LaTeX output for tables like in
#               http://cnts.uia.ac.be/conll2003/ner/example.tex
#            r: accept raw result tags (without B- and I- prefix;
#                                       assumes one word per chunk)
#            d: alternative delimiter tag (default is single space)
#            o: alternative outside tag (default is O)
# note:      the file should contain lines with items separated
#            by $delimiter characters (default space). The final
#            two items should contain the correct tag and the 
#            guessed tag in that order. Sentences should be
#            separated from each other by empty lines or lines
#            with $boundary fields (default -X-).

so I guess:
	
	idx word gold-pos gnt-pos
	
	with separator single space
	
	with "-X- -X- -X- -X-" as sentence boarder
	
- two steps necessary:
	- create eval file
	- call ./bin/conlleval -r < eval-file
	

	
I do now compute accuracy by my own. seems to be correct.
See class corpus/EvalConllFile.java
	
	
- Current results: with trained ptb3-training.conll
iw=0, #sent=392740, FLORS:

"resources/data/sancl-2012/sancl.labeled/gweb-newsgroups-dev.conll", "resources/eval/gweb-newsgroups-dev-flors.txt"
-> All pos: 22398 Correct: 19500 Accuracy: 87,06% (official: 89,14%, -2,08)

"resources/data/sancl-2012/sancl.labeled/gweb-reviews-dev.conll", "resources/eval/gweb-reviews-dev-flors.txt"
-> All pos: 27504 Correct: 24777 Accuracy: 90,09%  (official: 91,80%, -1,71)

"resources/data/sancl-2012/sancl.labeled/gweb-weblogs-dev.conll", "resources/eval/gweb-weblogs-dev-flors.txt"
-> All pos: 24025 Correct: 21831 Accuracy: 90,87% (official: 93,40%, -2,53)

"resources/data/sancl-2012/sancl.labeled/gweb-answers-dev.conll", "resources/eval/gweb-answers-dev-flors.txt"
-> All pos: 25180 Correct: 22044 Accuracy: 87,55% (official: 89,47%, -1,92)

"resources/data/sancl-2012/sancl.labeled/gweb-emails-dev.conll", "resources/eval/gweb-emails-dev-flors.txt"
-> All pos: 29131 Correct: 24898 Accuracy: 85,47% (official: 88,21%, 2,74)

"resources/data/english/ptb3-devel.conll", "resources/eval/ptb3-devel-flors.txt"
-> All pos: 40121 Correct: 38720 Accuracy: 96,51% (official: 96,29%, +0,3)

"resources/data/pbiotb/dev/english_pbiotb_dev.conll", "resources/eval/english_pbiotb_dev.txt"
-> All pos: 5017 Correct: 4472 Accuracy: 89,14% (official: 87,27%, +1,87)


PTB3-std: For current SOTA: http://aclweb.org/aclwiki/index.php?title=POS_Tagging_%28State_of_the_art%29

With model: resources/models/model_0iw38215sent_FTT_L2R_L2LOSS_SVC.txt

"resources/data/english/ptb3-std-devel.conll", "resources/eval/ptb3-std-devel-flors.txt"
-> All pos: 131808 Correct: 127434 Accuracy: 96,68%
"resources/data/english/ptb3-std-test.conll", "resources/eval/ptb3-std-test-flors.txt"
-> All pos: 129696 Correct: 125510 Accuracy: 96,77% (-0,34% cf. Flors, -0,78% cf. best)


With model: resources/models/model_25iw38215sent_TTT_L2R_L2LOSS_SVC.txt

"resources/data/english/ptb3-std-devel.conll", "resources/eval/ptb3-std-devel-flors.txt"
-> All pos: 131808 Correct: 110583 Accuracy: 83,90%
"resources/data/english/ptb3-std-test.conll", "resources/eval/ptb3-std-test-flors.txt"
-> All pos: 129696 Correct: 106930 Accuracy: 82,45%



Testing NER:
-> had to update corpus data foies as well!

eng.train.conll -> mapped to conll format via 

modefile: model_0iw-1sent_FTT_L2R_L2LOSS_SVC.txt
mapper.transcodeNERfile("resources/data/ner/eng.testb", "utf-8", "resources/data/ner/eng-testb.conll", "utf-8");


./conlleval < eval/eng-testa.txt 
processed 51578 tokens with 9408 phrases; found: 6155 phrases; correct: 5000.
accuracy:  97.40%; precision:  81.23%; recall:  53.15%; FB1:  64.25
              LOC: precision:  88.45%; recall:  90.47%; FB1:  89.45
             MISC: precision:  81.54%; recall:  79.07%; FB1:  80.29
              ORG: precision:  67.76%; recall:  75.24%; FB1:  71.31
              PER: precision:  84.52%; recall:  86.86%; FB1:  85.68
                X: precision:   0.00%; recall:   0.00%; FB1:   0.00
lt-pool-183:resources gune00$ ./conlleval < eval/eng-testb.txt 
processed 46666 tokens with 9332 phrases; found: 6133 phrases; correct: 4252.
accuracy:  95.45%; precision:  69.33%; recall:  45.56%; FB1:  54.99
              LOC: precision:  80.66%; recall:  86.03%; FB1:  83.26
             MISC: precision:  63.35%; recall:  71.65%; FB1:  67.25
              ORG: precision:  59.26%; recall:  67.25%; FB1:  63.00
              PER: precision:  71.46%; recall:  74.03%; FB1:  72.72
                X: precision:   0.00%; recall:   0.00%; FB1:   0.00

ModelInfo modelInfo = new ModelInfo("FLORS");
		int windowSize = 2;
		int numberOfSentences = 14000;
		int dim = 50;
		WordFeatures.withWordFeats=true;
		
lt-pool-183:resources gune00$ ./conlleval < eval/eng-testb.txt 
processed 46666 tokens with 9332 phrases; found: 6010 phrases; correct: 4353.
accuracy:  95.77%; precision:  72.43%; recall:  46.65%; FB1:  56.75
              LOC: precision:  80.62%; recall:  85.31%; FB1:  82.90
             MISC: precision:  68.58%; recall:  71.51%; FB1:  70.01
              ORG: precision:  60.43%; recall:  62.79%; FB1:  61.59
              PER: precision:  77.50%; recall:  85.65%; FB1:  81.37
                X: precision:   0.00%; recall:   0.00%; FB1:   0.00
lt-pool-183:resources gune00$ ./conlleval < eval/eng-testa.txt 
processed 51578 tokens with 9408 phrases; found: 6135 phrases; correct: 4912.
accuracy:  97.05%; precision:  80.07%; recall:  52.21%; FB1:  63.21
              LOC: precision:  86.51%; recall:  89.38%; FB1:  87.93
             MISC: precision:  78.23%; recall:  74.84%; FB1:  76.50
              ORG: precision:  66.20%; recall:  66.74%; FB1:  66.47
              PER: precision:  84.12%; recall:  91.48%; FB1:  87.65
                X: precision:   0.00%; recall:   0.00%; FB1:   0.00