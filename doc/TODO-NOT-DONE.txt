HIERIX: January 2016

- check to normalize/scale each component vector
	to unit length (compute length of vector, and then divide each component)
	I think I forgot it 
	-> I am doing it now on complete feature vector in 
		trainer.ProblemInstance.createProblemInstanceFromWindow(Window)
	-> try it on relevant component feature!
-> LEARN TO UNDERSTAND, why not necessary !

- *** take care of tokenization - currently just space

- Cluster from Jon Dehardy - clusterCat
- retrain clusters
	
- approximate matching for unknown words when looking up cluster map

- Other corpora for DE-POS and DE-NER
	- Unlabeled Mannheim, google books, ...

- other shape features
	- cf. doc/LiblinearNER.txt
	- I think that this could be a source for improvement, and hence, do some research here

- *** Use previous predicted tag as feature for POS/NER

- *** Extract character distribution from labeled data

- use other word embedding -> word2vec -> 
	but actually only used in NER -> so check, whether really usable
	maybe better clustering or other co-occurrence  models (WebQA?)
	- character and other embeddings

- POS tags as feature for NER
	- how can I define a kind of pipeline model for GNT ?


- Use of gazetteers:
	- instead of using it as feature, use it as postfilter as part of tagger.PostProcessor.java
	- use nemexa for exact match
	- learn to obtain optimal threshold for nemexf via regression analysis on training analysis
	- get NEs also from Wikidata
	
	- add to clusters entries which belong to 
		NE classes, e.g., cluserID 417 seems to cover first names
		define them as additional dictionaries and insert them when reading in clusterIds
		
- run GNT with suffix AND ngram by concatenating both into one file
	- DONE -> errors when concatenation -> I forgot to sort again after adding indices in 
		features.WordSuffixFeatureFactory.getAllKnownSubstringsForWord(String)
		
- compute all prefixes AND eventually combine with suffixes
	- or use as separate feature
	
- set un-frequent features to 0 -> cf. comments in MDP/doc/GN-TODO.txt

***
Feature learning:
- Idea: extract POS/NER dictionary of tagged tokens or sequences from training file
	- without any (?) preprocessing of token spelling
	
- Extract and apply dictionary on training file
	- should re-cover training file
	- analyze positive matches, partial matches and negative matches
	- check autoencoder for automatic feature generation

***
Additional applications:

- GNT for lemmatization
	
- GNT also for chunking
	- should be very simple, but would eventually need POS tagging

- GNT for semantic role labeling/tagging

- GNT for relation extraction
	- maybe use a specific BIO-like annotation schema
	- check semantic role annotation
	
***
For software package:
- create property file -> DONE
- save and move it to model file folder -> DONE
- add FLORS accuracy to evaluation script
- create input file for TORCH
- generated compressed folder of all data that is necessary for tagging
	- store model file also under taggerName feature dir
	- then compress model file and feature files -> very useful !
	- see also corpus.WikiPediaConllReader.getBufferedReaderForCompressedFile(String)
	
- define feature instances ala ClearNLP via XML
-> how to integrate feature extraction approach dynamically ?

- define logger
- define parallel processing
- do splitting according to
	

	

	