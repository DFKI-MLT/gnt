August, 2016:


My idea of defining a cascade of liblinear learners seems to be 
what is known as recurrent Machine Learning;

It should be possible to integrate the labels from the window elements (as well as the predicted onces)
from left to right (only ??)

Steps:

Training:

- define a feature for predictedLabels
	feature vector can be built from known label set
- initialize data.OffSets.initializeOffsets(Alphabet, int)
- label set should be known anyway so no need to load it during training and tagging
	- but add as predicate to Alphabet
	- unknown labels are bascially ignore and not used for features
	
- adapt trainer.TrainerInMem.constructProblem(boolean, boolean)
	- create predictedLabels feature for left window context in data.Window.fillWindow(boolean, boolean)
	- data.Window.createWordFeaturesRecurrent(Sentence, String, int, int, boolean, boolean)
		- if isWithLabelFeats() then add predicted labels
	-> NOT necessary: just use the current value of the label for word i, 
		because either it has been assigned a new label or the dummy value
	- set off sets via features.WordFeatures.setOffSets(Alphabet, OffSets)
	- add new feature to features.WordFeatures and adapt like clusterId features
	- adjust feature vector in trainer.ProblemInstance.createProblemInstanceFromWindow(Window)

Tagging:
- labels are already loaded
- set up data.OffSets.initializeOffsets(Alphabet, int)
- I think it should be the same as in training, because labels are predicted left-to-right
	and overwrite the dummy cat
	
Das klappt irgendwie  nicht:
- setzte ich die predicted labels wirklich richtig ?

Technisch geht es, aber wenn ich dummy labels nehme beim Taggen in data.Data.generateSentenceObjectFromConllLabeledSentence(List<String[]>)
kommt sehr schlechtes ergebnis raus;
Das heisst ich muss noch mal genauer den Informationfluss checken