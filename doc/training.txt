2. Training of POS tagger and liblinear
		
Java object structure:

- define trainer class
	has a corpus object
	defines window size l
	loads or creates distributed vectors
	loads or creates pre-computed suffixes
	loads or creates pre-computed shape features

-> 	I can combine all in a class Alphabet initialized with corpus
	- training word2index mapping
	- label set -> incremental
	- distributed word factory -> loaded initially
	- suffix features 	-> incremental
	- shape features	-> incremental
	
	process conll training file and create sentence internal forms: list of pairs (word, pos)
	I need to define a label2num num2label file
	
	for each sentence, perform windows based processing, with pre-specified window size
	fills feature vector parts  for each word
	writes out liblinear model file (as gzipped)
	
	see http://stats.stackexchange.com/questions/61328/libsvm-data-format
	
Creating training data file:
		
Current approach:
- load word vectors, suffixes, shapes
	- seems that offline mode is ok, because then I can do experiments with ranked suffixes
	
-> but running and creating all the data before training starts is also possible, because creating the
   features is not so time consuming (as whole training later)
-> DONE

- initialize Alphabet -> DONE

- initialize offSets -> DONE

- loop through training sentences incrementally
	- save label set in file

- create training window instance for each word:
	- get label and create label map -> DONE -> in class Data
	- get words of window -> DONE
	- initialize window and store all window frames in training instances
	
- construct problem
	- 	initialize problem
	-	iterate through all training instances
		- for each word create a tokenInstance consisting of four list of indices
		- get left and right distributed word vector in form of index:value
		- compute suffixes, which also incrementally extends suffix factory in form of index:1
		- compute shapes, which also incrementally extends suffix factory  in form of index:1
		- add an offset to relative indices
	- 	map each filled window to problem instance and clean it
		- feature vector size is given by means of maxFeatureSize
		- add index of label and create feature nodes
	
- run trainer:
	- initialize liblinear with problem and parameters
	- run learner
	- save model
	
- finally also write out problem relevant values into file and label set mapping
	- parametrize trainer with model file string
	- parameter setting for learner
	- modelfile: has all settings for problem
	- save label set
	- save windowSize


NOTES on Alphabet:
- load preprocessed feature data (distributed word vectors, suffixes, shapes)
- this also gives me the max number of different feature values maxFeatureSize from which 
	- I can compute the offsets of each window element
	- max number of different features
- when filling the components of each window token, I determine
	- firstly the relative index of each non-zero feature using the alphabet
	- and then compute the absolute index using the offsets
- in this way, the features indices are enumerate within a window in increasing order
	and only the non-zero feature are collected, which gives sparse window elements
		
NOTES on in memory training
	-> no need to save training instances in file
	-> create window frame instances (non filled windows)
	-> gives me training instances size needed for init problem
	-> seems to be fast enough
	-> then fill window, create problem instance and add to problem
		and finally clean window to save memory (garbage collection)
		define a method Window.clean()

-> DONE; looks good ! get my first trained models ! 11.9.2015

-> NEXT
	- store all file in compressed folder automatically

Before training, create features:

- eventually update corpus and create sentences only files using ConllMapper.java
- create iw
- create shapes 
- create suffix
- create word vectors -> mostly done when training is done
	
Activation of features: if one is set to false, feature is deactivated -> also use this in model file name
public static boolean withWordFeats = true;
public static boolean withShapeFeats = true;
public static boolean withSuffixFeats = true;

- now, when running:

with w=2, d=0, m=GNT, s=39274 und features activated=FTT
training time: 1801235
model saving time: 38036
model file: resources/models/model_0iw39274sent_FTT_L2R_LR.txt (467MB)

same, but m=FLORS:
System time (msec): 673170
Save  model file: resources/models/model_0iw39274sent_FTT_L2R_L2LOSS_SVC.txt
System time (msec): 39302

training with new ptb3 training set of 39826 sentences: 11.5470167 minutes

training with new ptb3-std-training setting: 38215 (of 38219): 10.5294167 minutes


EFFICIENCY:

Memory needed:
n * m * 8 + m: 
average vector length * #window instances * 8 (double) + #window instances

PROBLEM:

when sentence count is -1 (max=39278) or > 39274; 
-> I get lib-linear error that indices in problem instances are not correctly ordered 
-> DONE ->  
	OffSets.windowVectorSize = (tokenVectorSize * (windowSize*2+1))+(windowSize*2+1);

Everything works, but I cannot reproduce FLORS result so far; but the trend is ok;

when not using distributed word vectors, I still have at least one feature length of 1

HIERIX
Map current absolute index to integer enumeration 1, 2, ...
This should reduce the necessary size of problem.n
However, when done so I need to memorize the mappping as part of Alphabet for testing phase later
and for restroing the absolute indices

