2. Training of POS tagger and liblinear
		
Java object structure:

- define trainer class
	has a corpus object
	defines window size l
	loads or creates distributed vectors
	loads or creates pre-computed suffixes
	loads or creates pre-computed shape features

-> 	I can combine all in a class Alphabet initialized with corpus
	- training word2index mapping
	- label set -> incremental
	- distributed word factory -> loaded initially
	- suffix features 	-> incremental
	- shape features	-> incremental
	
	process conll training file and create sentence internal forms: list of pairs (word, pos)
	I need to define a label2num num2label file
	
	for each sentence, perform windows based processing, with prespecified window size
	fills feature vector parts  for each word
	writes out liblinear model file (as gzipped)
	
	see http://stats.stackexchange.com/questions/61328/libsvm-data-format
	
Creating training data file:
		
Current approach:
- load word vectors, suffixes, shapes
	- seems that offline mode is ok, because then I can do experiments with ranked suffixes
	
-> but running and creating all the data before training starts is also possible, because creating the
   features is not so time consuming (as whole training later)
-> DONE

- initialize Alphabet -> DONE

- initialize offSets -> DONE

- loop through training sentences incrementally
	- save label set in file

- create training window instance for each word:
	- get label and create label map -> DONE -> in class Data
	- get words of window -> DONE
	- initialize window and store all window frames in training instances
	
	
- construct problem
	- 	initialize problem
	-	iterate through all training instances
		- for each word create a tokenInstance consisting of four list of indices
		- get left and right distributed word vector in form of index:value
		- compute suffixes, which also incrementally extends suffix factory in form of index:1
		- compute shapes, which also incrementally extends suffix factory  in form of index:1
		- add an offset to relative indices
	- 	map each filled window to problem instance and clean it
		- feature vector size is given by means of maxFeatureSize
		- add index of label and create feature nodes
	
- run trainer:
	- initialize liblinear with problem and parameters
	- run learner
	- save model
	
- finally also write out problem relevant values into file and label set mapping
	- parametrize trainer with model file string
	- parameter setting for learner
	- modelfile: has all settings for problem
	- save label set
	- save windowSize


NOTES on Alphabet:
- load preprocessed feature data (distributed word vectors, suffixes, shapes)
- this also gives me the max number of different feature values maxFeatureSize from which 
	- I can compute the offsets of each window element
	- max number of different features
- when filling the components of each window token, I determine
	- firstly the relative index of each non-zero feature using the alphabet
	- and then compute the absolute index using the offsets
- in this way, the features indices are enumerate within a window in increasing order
	and only the non-zero feature are collected, which gives sparse window elements

NOTES: about writing out training instances
	-> define a out buffer as part of Corpus and initialize with filename;
	-> open that buffer before training starts, and close it after.
	-> DONE: define a new ModelFiles.java class
	-> BUT files are getting really huge !
	-> file writing time: dim:50, #sent: 5000 -> 2.44 minutes, dim:250, #sent: 5000 -> 29.59 minutes
	-> with the approach abve, splitting should also be possible 
		-> create pos-many different files
		-> split word vectors into n-parts and save n-different files
		
NOTES: possible alternative -> no need to save training instances in file
	-> create window frame instances (non filled windows)
	-> gives me training instances size needed for init problem
	-> seems to be fast enough
	-> then fill window, create problem instance and add to problem
		and finally clean window to save memory (garbage collection)
		define a method Window.clean()

-> DONE; looks good ! get my first trained models ! 11.9.2015

-> NEXT
	- store all file in compressed folder automatically
	- run larger experiments, when tagging done

Training:
- defined training and run it using FLORS learner setting 
	new Parameter(SolverType.L2R_LR, 1.0, 0.01); -> DONE ! used Xmx6g
	-> time for constructing model iw=50 and examples=5000: 35.3328333 minutes
	-> saving model in: /Users/gune00/data/wordVectorTests/testModel__L2R_LR.txt -> time 25 seconds (207MB)
	-> TOTAL  System time (msec): 2156062 -> 35.9343667 minutes
	
- defined training and run it using MDP learner setting 
	new Parameter(SolverType.MCSVM_CS, 0.1, 0.3); but bias not used
	-> time for constructing model iw=50 and examples=5000: 19.57455 minutes
	-> saving model in: /Users/gune00/data/wordVectorTests/testModel__L2R_LR.txt -> time 12 seconds (59MB)
	-> TOTAL  System time (msec): 1174473 -> 19.57455 minutes


EFFICIENCY:

-> I will first go with iw=50 and examples=5000 for testing complete system

Processing experiments:
All training instances
- with iw=100: 0.35705 minutes
- with iw=200: 0.499816667 minutes
- with iw=300: 0.659533333 minutes

Used in Flors:
- with iw=250: 0.575133333 minutes
- with iw=500: 0.9153 minutes

Memory needed:
n * m * 8 + m (laut Bernd)
average vector length * #window instances * 8 (double) + #window instances

iw=500, #sentences=39278 ->
2291*958150*8+958150 -> 17.5619313 gigabytes

iw=50, #sentences=5000 -> 
365*122983*8+122983 -> 0.359233343 gigabytes

iw=250, #sentences=5000
1303*122983*8+122983 -> 1.28209777 gigabytes
 
 