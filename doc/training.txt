2. Training of POS tagger and liblinear
		
see http://stats.stackexchange.com/questions/61328/libsvm-data-format
	
Creating training data file:
		
- load word vectors, suffixes, shapes
	- seems that offline mode is ok, because then I can do experiments with ranked suffixes
	
-> but running and creating all the data before training starts is also possible, because creating the
   features is not so time consuming (as whole training later)
-> DONE

- initialize Alphabet -> DONE

- initialize offSets -> DONE

Training phase:
- loop through training sentences incrementally
	- save label set in file

- create training window instance for each word:
	- get label and create label map -> DONE -> in class Data
	- get words of window -> DONE
	- initialize window and store all window frames in training instances
	
- construct problem
	- 	initialize problem
	-	iterate through all training instances
		- for each word create a tokenInstance consisting of four list of indices
		- get left and right distributed word vector in form of index:value
		- compute suffixes, which also incrementally extends suffix factory in form of index:1
		- compute shapes, which also incrementally extends suffix factory  in form of index:1
		- add an offset to relative indices
	- 	map each filled window to problem instance and clean it
		- feature vector size is given by means of maxFeatureSize
		- add index of label and create feature nodes
	
- run trainer:
	- initialize liblinear with problem and parameters
	- run learner
	- save model
	
- finally also write out problem relevant values into file and label set mapping
	- parametrize trainer with model file string
	- parameter setting for learner
	- modelfile: has all settings for problem
	- save label set
	- save windowSize

- final model exists of following data needed for doing tagging later:
	1. suffixList.txt -> the number-suffix mapping computed from the labeled training corpus
	2. shapeList.txt -> the number-bitvector mapping of shape features computed from the labeled training corpus
	3. iwDIM.txt -> indicator words used to create the distributed vector dimension
	4. vocContentDIM.txt -> the distributed vectors; keeping left and right vectors separated by ###
	5. vocFile.txt -> the mapping of words to distributed vectors as defined by means of all used unlabeled/labeled corpus
	6. labelSet.txt -> the number-label mapping as defined by means of the training corpus
	7. wordSet.txt -> the number-word mapping of all words of the labeled training corpus; used for computing unknown word tagging accuracy
	8. the model file
	


NOTES on Alphabet:
- load preprocessed feature data (distributed word vectors, suffixes, shapes)
- this also gives me the max number of different feature values maxFeatureSize from which 
	- I can compute the offsets of each window element
	- max number of different features
- when filling the components of each window token, I determine
	- firstly the relative index of each non-zero feature using the alphabet
	- and then compute the absolute index using the offsets
- in this way, the features indices are enumerate within a window in increasing order
	and only the non-zero feature are collected, which gives sparse window elements
		
NOTES on in memory training
	-> no need to save training instances in file
	-> create window frame instances (non filled windows)
	-> gives me training instances size needed for init problem
	-> seems to be fast enough
	-> then fill window, create problem instance and add to problem
		and finally clean window to save memory (garbage collection)
		define a method Window.clean()

-> DONE; looks good ! get my first trained models ! 11.9.2015

-> NEXT
	- store all file in compressed folder automatically

Before training, create features:

- eventually update corpus and create sentences only files using ConllMapper.java
- create iw
- create shapes 
- create suffix
- create word vectors -> mostly done when training is done
	
Activation of features: if one is set to false, feature is deactivated -> also use this in model file name
public static boolean withWordFeats = true;
public static boolean withShapeFeats = true;
public static boolean withSuffixFeats = true;

- now, when running:

with w=2, d=0, m=GNT, s=39274 und features activated=FTT
training time: 1801235
model saving time: 38036
model file: resources/models/model_0iw39274sent_FTT_L2R_LR.txt (467MB)

same, but m=FLORS:
System time (msec): 673170
Save  model file: resources/models/model_0iw39274sent_FTT_L2R_L2LOSS_SVC.txt
System time (msec): 39302

training with new ptb3 training set of 39826 sentences: 11.5470167 minutes

training with new ptb3-std-training setting: 38215 (of 38219): 10.5294167 minutes


EFFICIENCY:

Memory needed:
n * m * 8 + m: 
average vector length * #window instances * 8 (double) + #window instances

PROBLEM:

when sentence count is -1 (max=39278) or > 39274; 
-> I get lib-linear error that indices in problem instances are not correctly ordered 
-> DONE ->  had to add additional window-size (windowSize*2+1)
	OffSets.windowVectorSize = (tokenVectorSize * (windowSize*2+1))+(windowSize*2+1);

Everything works, but I cannot reproduce FLORS result so far; but the trend is ok;

when not using distributed word vectors, I still have at least one distributed feature length of 1

