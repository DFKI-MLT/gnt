2. Training of POS tagger and liblinear
		
Java object structure:

- define trainer class
	has a corpus object
	defines window size l
	loads or creates distributed vectors
	loads or creates pre-computed suffixes
	loads or creates pre-computed shape features

-> 	I can combine all in a class Alphabet initialized with corpus
	- training word2index mapping
	- label set -> incremental
	- distributed word factory -> loaded initially
	- suffix features 	-> incremental
	- shape features	-> incremental
	
	process conll training file and creates 
	training instance: list of pairs (word, pos)
	I need to define a label2num num2label file
	
	for each sentence, performs windows based processing, with prespecified window size
	fills feature vector parts  for each word
	writes out liblinear model file (as gzipped)
	
	what to do with the real feature values of word vector?
	what to do with zero elements ? 
	and splitting?
	
	see http://stats.stackexchange.com/questions/61328/libsvm-data-format
	
Creating training data file:
		
Possible approach:
- load word vectors, suffixes, shapes
	- seems that offline mode is ok, because then I can do experiments with ranked suffixes
-> DONE
- initialize Alphabet -> DONE
- initialize offSets -> DONE
- loop through training sentences incrementally
- create training instance for each word:
	- get label and create label map -> DONE -> in class Data
	- get words of window -> DONE
	- for each word create a tokenInstance consisting of four list of indices
		- get left and right distributed word vector in form of index:value
		- compute suffixes, which also incrementally extends suffix factory in form of index:1
		- compute shapes, which also incrementally extends suffix factory  in form of index:1
	-> DONE
	
- the indices above are only relative 
- so I need to add an offset:
	- the offset depends on the size of the length of word vectors, suffix vector, and shape vector and the position of the word:
	- if precomputed offset is used, offsets can be computed directly, else offset can only be computed at the end
- when I have the offsets, I can compute the absolute index-feature map as well which is used by liblinear
-> DONE -> actually done incrementally for each newly create window
	Approach:
		- currently, I follow an offline approach:
		- load preprocessed feature data (distributed word vectors, suffixes, shapes)
		- this also gives me the max number of different feature values from which 
			- I can compute the offsets of each window element
			- max number of different features
		- when filling the components of each window token, I determine
			- firstly the relative index of each non-zero feature using the alphabet
			- and then compute the absolute index using the offsets
		- in this way, the features indices are enumerate within a window in increasing order
			and only the non-zero feature are collected, which gives sparse window elements
	
- transform each window into a problem
	- add index of label
	- create feature nodes
	
-> IT is clear that it is going slow if I have to store windows in memory.
-> so, try first storing them in a file
-> OR use smaller data ???

-> and eventually already split them ?

- and later also by the predictor
 
 