2. Training of POS tagger and liblinear

- use conll dependency treebanks or SVM format  ->
	each line corresponds to a sequence of tokens, which are eventually labeled
	w1/t1, w2/t2, ..., wn/tn


- the idea is in principle very simple - iterate through sentence and create per token:
	training: define a feature vector of
		label feat:num ... feat:num
	testing/predicting
		feat:num ... feat:num
		
- hence the major steps are the encoding/decoding of the training/input data 

- following MDP, I would need classes for:
	- Alphabet: string2num and num2string mappings -> DONE via corpus.DistributedWordVectorFactory
		- for all labels
		- for all words
		
	- FeatureExtractor -> a class that keeps the feature templates
		for computing suffixes -> DONE
		for computing shape features -> DONE
	
- initialize: -> DONE
	read in word vectors and indicatorWords
	initialize bijective indexes
	
- read in training corpus line wise (assuming each line corresponds to an training/testing instance)

- read in training file:
	- for each sentence
		- create word data element:
			- word
			- label
			- word vector left
			- word vector right
			- compute suffixes
			- compute shape
			
		- NOTE: keep the indexes so that symbolic input/output can be reconstructed, cf. http://deeplearning.net/tutorial/rnnslu.html
		
Java object structure:

- define trainer class
	has a corpus object
	loads or creates distributed vectors
	loads or creates pre-computed suffixes
	loads or creates pre-computed shape features
	defines window size l
	
	process conll training file and creates 
	training instance: list of pairs (word, pos)
	I need to define a label2num num2label file
	
	for each sentence, performs windows based processing, with prespecified window size
	fills feature vector parts  for each word
	writes out liblinear model file (as gzipped)
	
	what to do with the real feature values of word vector?
	what to do with zero elements ? 
	and splitting?
	
	see http://stats.stackexchange.com/questions/61328/libsvm-data-format
	
Creating training data file:
- I need to collect all labels and map them to integers
- I need to collect all features, concatenate them and map them to integers
- Thus I can define an alphabet class with
	- labelMap: label-index bijection
	- featureMap: 
		- 	this is a concatenation of the features from word vector, suffix, shapes
			and for each word in the window
		-	to use the mappings from the feature generation functions, I need to keep an offset
		- I need to multiple the features because of window site, e.g., shape_feature_n_for_word_1
		
Possible approach:
- load word vectors, which also instantiate training vocabulary, because training set is part of the word vectors
- create classes for suffix and shap features
- loop through training sentences incrementally
- create training instance for each word:
	- get label and create label map -> can be done incrementally
	- get words of window
	- for each word create a tokenInstance consisting of three list of indices
		- get left and right distributed word vector in form of index:value
		- compute suffixes, which also incrementally extends suffix factory in form of index:1
		- compute shapes, which also incrementally extends suffix factory  in form of index:1
	- the indices above are only relative 
	- so I need to add an offset:
		- the offset depends on the size of the length of word vectors, suffix vector, and shape vector and the position of the word:
		- if precomputed offset is used, offsets can be computed directly, else offset can only be computed at the end
	- when I have the offsets, I can compute the absolute index-feature map as well which is used by liblinear