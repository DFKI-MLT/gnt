2. Training of POS tagger and liblinear
		
Java object structure:

- define trainer class
	has a corpus object
	defines window size l
	loads or creates distributed vectors
	loads or creates pre-computed suffixes
	loads or creates pre-computed shape features

-> 	I can combine all in a class Alphabet initialized with corpus
	- training word2index mapping
	- label set -> incremental
	- distributed word factory -> loaded initially
	- suffix features 	-> incremental
	- shape features	-> incremental
	
	process conll training file and create sentence internal forms: list of pairs (word, pos)
	I need to define a label2num num2label file
	
	for each sentence, perform windows based processing, with prespecified window size
	fills feature vector parts  for each word
	writes out liblinear model file (as gzipped)
	
	see http://stats.stackexchange.com/questions/61328/libsvm-data-format
	
Creating training data file:
		
Current approach:
- load word vectors, suffixes, shapes
	- seems that offline mode is ok, because then I can do experiments with ranked suffixes
	
-> but running and creating all the data before training starts is also possible, because creating the
   features is not so time consuming (as whole training later)
-> DONE

- initialize Alphabet -> DONE

- initialize offSets -> DONE

- loop through training sentences incrementally
	- save label set in file

- create training window instance for each word:
	- get label and create label map -> DONE -> in class Data
	- get words of window -> DONE
	- initialize window and store all window frames in training instances
	
	
- construct problem
	- 	initialize problem
	-	iterate through all training instances
		- for each word create a tokenInstance consisting of four list of indices
		- get left and right distributed word vector in form of index:value
		- compute suffixes, which also incrementally extends suffix factory in form of index:1
		- compute shapes, which also incrementally extends suffix factory  in form of index:1
		- add an offset to relative indices
	- 	map each filled window to problem instance and clean it
		- feature vector size is given by means of maxFeatureSize
		- add index of label and create feature nodes
	
- run trainer:
	- initialize liblinear with problem and parameters
	- run learner
	- save model
	
- finally also write out problem relevant values into file and label set mapping
	- parametrize trainer with model file string
	- parameter setting for learner
	- modelfile: has all settings for problem
	- save label set
	- save windowSize


NOTES on Alphabet:
- load preprocessed feature data (distributed word vectors, suffixes, shapes)
- this also gives me the max number of different feature values maxFeatureSize from which 
	- I can compute the offsets of each window element
	- max number of different features
- when filling the components of each window token, I determine
	- firstly the relative index of each non-zero feature using the alphabet
	- and then compute the absolute index using the offsets
- in this way, the features indices are enumerate within a window in increasing order
	and only the non-zero feature are collected, which gives sparse window elements

NOTES: about writing out training instances
	-> define a out buffer as part of Corpus and initialize with filename;
	-> open that buffer before training starts, and close it after.
	-> DONE: define a new ModelFiles.java class
	-> BUT files are getting really huge !
	-> file writing time: dim:50, #sent: 5000 -> 2.44 minutes, dim:250, #sent: 5000 -> 29.59 minutes
	-> with the approach abve, splitting should also be possible 
		-> create pos-many different files
		-> split word vectors into n-parts and save n-different files
		
NOTES: possible alternative -> no need to save training instances in file
	-> create window frame instances (non filled windows)
	-> gives me training instances size needed for init problem
	-> seems to be fast enough
	-> then fill window, create problem instance and add to problem
		and finally clean window to save memory (garbage collection)
		define a method Window.clean()

-> DONE; looks good ! get my first trained models ! 11.9.2015

-> NEXT
	- store all file in compressed folder automatically
	- run larger experiments, when tagging done

Status Training:
- defined training and run it using FLORS learner setting 
	new Parameter(SolverType.L2R_LR, 1.0, 0.01); -> DONE ! used Xmx6g
	-> time for constructing model iw=50 and examples=5000: 35.3328333 minutes
	-> saving model in: /Users/gune00/data/wordVectorTests/testModel__L2R_LR.txt -> time 25 seconds (207MB)
	-> TOTAL  System time (msec): 2156062 -> 35.9343667 minutes
	
- defined training and run it using MDP learner setting 
	new Parameter(SolverType.MCSVM_CS, 0.1, 0.3); but bias not used
	-> time for constructing model iw=50 and examples=5000: 19.57455 minutes
	-> saving model in: /Users/gune00/data/wordVectorTests/testModel_MCSVM_CS.txt -> time 12 seconds (59MB)
	-> TOTAL  System time (msec): 1174473 -> 19.57455 minutes
	
	-> iw=50 and examples=10000 -> 2068044 msec -> 34.4674 minutes
	Save  model file: /Users/gune00/data/wordVectorTests/testModel10k_MCSVM_CS.txt -> time ca. 12 seconds
	
	-> iw=100 and examples=10000 -> 4109242 msec -> 68.4873667 minutes
	Save  model file: /Users/gune00/data/wordVectorTests/testModel100iw10k_MCSVM_CS.txt -> 162 seconds (ca. 60MB)
	-> it seems that when using modelInfo=MDP, then size of model is similar independent of parameters (and so the load time)
	
	-> but also for L2R_LR it growth moderately
	
Activation of features: if one is set to false, featuer is deactivated -> also use this in model file name
public static boolean withWordFeats = true;
public static boolean withShapeFeats = true;
public static boolean withSuffixFeats = true;

- now, when running:

with w=2, d=0, m=GNT, s=39274 und features activated=FTT
training time: 1801235
model saving time: 38036
model file: resources/models/model_0iw39274sent_FTT_L2R_LR.txt (467MB)

same, but m=FLORS:
System time (msec): 673170
Save  model file: resources/models/model_0iw39274sent_FTT_L2R_L2LOSS_SVC.txt
System time (msec): 39302



EFFICIENCY:

Memory needed:
n * m * 8 + m (laut Bernd)
average vector length * #window instances * 8 (double) + #window instances

PROBLEM:

when sentence count is -1 (max=39278) or > 39274; 
-> I get lib-linear error that indices in problem instances are not correctly ordered 

Everything works, but I get not reproduce FLORS result:

- I get worse results for english-test and domain test:
	- in both cases I tag very often words with -RRB-, -LRB-
	- STRANGE: I predict pos tags which are not trained, e-g-, -RRB-, -LRB-, NFP, FW