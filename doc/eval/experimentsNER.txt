Current results:

English:

CoNLL-2003:
Devel: F1=91.75		 Test: 	F1=87.17
(Words/sec: 57822)			 (Words/sec: 94849)
Best 2016 (DL)
Devel:  F1=94,03	 Test: F1=91,20

Missing: 2,28 				4,03	

Deutsch:

CoNLL-2003
Devel: F1=69.07		 Test: 	F1=70.87
(Words/sec: 51593)			 (Words/sec: 86541)
Best 2016 (DL)
Devel:  F1=79.8	 	Test: F1=78.2

Missing: 10,73 				7,5


Konvens-2014
Devel: F1=73.34		 Test: 	F1=73.91
(Words/sec: 43416)			 (Words/sec: 65015)
Best 2016 (DL)
Devel:  F1=??	 	Test: F1=79,08

Missing: ?? 				5,17


Genia-Types from NLPBA challenge, 2004
NLPBA, 2004 data:

Test:		F1=66,96
			(Words/sec: 74184)

Best 2008: 	F1=77,6

Missing:	10,64%

HIERIX:

I have additional NE-conll data
	- improve training so that it can work with many files


apply GntNER on NL and ES conll data
data/conll2002

Check features:
add prefixes as features
add POS tags
add phrase clusters
add Jon's clusters


NER lexicons:
	- although I have word clusters, it might be too small
	- so very large NE lists could make sense
	- or retraining of Marmot cluster using own NER relevant data files

HOW to add gazeetteers to GNT 
	- as post processor
	- e.g., correcting assign labels as in with twitter tags?
	- as features like word classes
	
If I want to stick only on supervised data and unsupervised data (clusters, embeddings)
and want to avoid feature engineering, what can I do?
- extract sort of character distribution from labeled data to use as shape features
- used previous tagging decisions to model some sort of grammar
	-> both is considered important also in NN-based NER, cf. /Users/gune00/dfki/DeepLearning/NER/NAACL-2016.pdf
	-> BUT, how can this be done as part of liblinear?
	-> can I use a kind of second layer liblinear approach?





