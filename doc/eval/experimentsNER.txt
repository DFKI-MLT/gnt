Current results:

English:

CoNLL-2003:
Devel: F1=91.75		 Test: 	F1=87.17 (trained with train and testa: 87.47%)
(Words/sec: 57822)			 (Words/sec: 94849)
Best 2016 (DL), https://transacl.org/ojs/index.php/tacl/article/view/792/202
Devel:  F1=94,03	 Test: F1=91,62

Missing: 2,28 				4,45	(4,15)

Deutsch:

CoNLL-2003
Devel: F1=69.07		 Test: 	F1=70.87 (trained with train and testa: 73.43%)
(Words/sec: 51593)			 (Words/sec: 86541)
Best 2016 (DL)
Devel:  F1=79.8	 	Test: F1=78.2

Missing: 10,73 				7,5 (4,77)


Konvens-2014
Devel: F1=73.34		 Test: 	F1=73.91 (training with train and devel: no effect)
(Words/sec: 43416)			 (Words/sec: 65015)
Best 2016 (DL)
Devel:  F1=??	 	Test: F1=79,08

Missing: ?? 				5,17


Genia-Types from NLPBA challenge, 2004
NLPBA, 2004 data:

Test:		F1=66,96
			(Words/sec: 74184)

Best 2008: 	F1=77,6

Missing:	10,64%

HIERIX:

It seems that if sentence is all capitals, then performance is lower.
How can I make use of statistics to improve performance ?

I have additional NE-conll data
	- improve training so that it can work with many files
	
I have the ontonotes version 5.0 corpus
- /Volumes/data1/LDC/ontonotes-release-5.0
- see /Users/gune00/data/ontonotes-release-5.0 for an example


apply GntNER on NL and ES conll data
data/conll2002

Check features:
add prefixes as features
add POS tags
add phrase clusters
add Jon's clusters


NER lexicons:
	- although I have word clusters, it might be too small
	- so very large NE lists could make sense
	- or retraining of Marmot cluster using own NER relevant data files

HOW to add gazeetteers to GNT 
	- as post processor
	- e.g., correcting assign labels as in with twitter tags?
	- as features like word classes
	
If I want to stick only on supervised data and unsupervised data (clusters, embeddings)
and want to avoid feature engineering, what can I do?
- extract sort of character distribution from labeled data to use as shape features
- used previous tagging decisions to model some sort of grammar
	-> both is considered important also in NN-based NER, cf. /Users/gune00/dfki/DeepLearning/NER/NAACL-2016.pdf
	-> BUT, how can this be done as part of liblinear?
	-> can I use a kind of second layer liblinear approach?





