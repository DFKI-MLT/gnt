Current results:

Test on DE conll2003:
-> best result: 68,49 F1
-> Speed: ~47T tokens/sec; training: ~10 minutes

-> ablation tests (testb):
	cluster and 
	unlabeled features:	-10,42
	cluster features:	-6,87
	unlabeled features: -1,71 -> when modelinfo MDP: 67.89% -> -0,6% -> training = 8 sec.
	suffix features:	-7,08	
	shape features:		-1,02
	
-> combing training and development data: 
	about 70,50% F1 -> +2,45% F1 (but not usable for EN)
	when modelInfo MDP -> training time: 10 sec.

Test on EN conll2003:
-> best result: 83.29 F1
-> Speed: ~47T tokens/sec; training: ~10 minutes

-> ablation tests (testb):
	cluster and 
	unlabeled features:	-8,99
	cluster features:	-5,58	
	unlabeled features:	-0,56
	suffix features:	-2,7	
	shape features:		-2,47

-> 	when running modelInfo "MDP" I am loosing 1%F1, 
	but training much faster and model files are much smaller!

(Basically about -10% F1 for DE and EN)

-> with MDP and BILOU: 84.41% acc on test (using FLORS, 83,80%)
	
Test on DE Konvens2014 - only outer NE span classes:
-> best results: 69,89% F1
(about -9,19% compared to best, -3,35% compared with 3rd)
-> Speed: ~35T tokens /sec. ; training: ~0,61 minutes

Used features:

-	normalized text: lower case
-	no digit normalization
-	window size: 2
-	no subsampling
-	50 dim indicator words
-	all but only conll-data as unlabeled data
-	shape feature methods from POS tagger for EN and DE
-	all suffix features from training
-	1000 clusterIDs from Marmot
	- NOTE: Marmot cluserID are based on digit-normalized wiki-dumps

For possible other features:
https://github.com/clir/clearnlp-guidelines/blob/master/md/components/named_entity_recognition.md

Testing:

- using windowSize=1 hurts !
- using windowSize=3 neutral

- neither 1grams nor 3grams bring anything

- smaller suffixes
-> initial test with suffix <= 5 brings a bit for EN, but hurts for DE
-> suffix <= 3 hurts for EN and DE

- generalization of number/date expressions
-> not useful so far (cf. doc/corpus.txt)

- it seems that only dimension around 50 brings something
-> also Turian et al. 2010 only use 50-dims for their word embedding stuff

- Actually, it is unclear, whether GNT features are usable for NER
-> check again paper from Tkachenko & Simanovsky


extend training with testa and check how good results are on testb
-> indicates whether extending training would be helpful:

For English:
cat eng-train.conll eng-testa.conll > eng-traintesta.conll
cat eng-train-sents.txt eng-testa-sents.txt > eng-traintesta-sents.txt
For German:
cat deu-train.conll deu-testa.conll > deu-traintesta.conll
cat deu-train-sents.txt deu-testa-sents.txt > deu-traintesta-sents.txt

-> helpful for DE (+2,45% F1) but not for EN


Shape features 
	-> Try out: remove/deactivate suffix features as part of shape-features
		because they already part of suffix features
	-> DONE: brings a bit for POS and NER, so I deactivate that part of the features
	-> cf. features.WordShapeFeature.createShapeVectorFromWord(String, int)
	
still I am missing about 8%F1 for CONLL

Evaluation with Konvens 2014 DE NER data: DONE

BILOU schema:
- brings for EN about 1% F1, but for German it hurts

I have additional NE-conll data
	- improve training so that it can work with many files

HIERIX:

add prefixes as features

NER lexicons:
	- although I have word clusters, it might be too small
	- so very large NE lists could make sense
	- or retraining of Marmot cluster using own NER relevant data files

HOW to add gazeetteers to GNT 
	- as post processor
	- e.g., correcting assign labels as in with twitter tags?
	
If I want to stick only on supervised data and unsupervised data (clusters, embeddings)
and want to avoid feature engineering, what can I do?
- extract sort of character distribution from labeled data to use as shape features
- used previous tagging decisions to model some sort of grammar
	-> both is considered important also in NN-based NER, cf. /Users/gune00/dfki/DeepLearning/NER/NAACL-2016.pdf
	-> BUT, how can this be done as part of liblinear?
	-> can I use a kind of second layer liblinear approach?





