October, 2015:

- using windowSize=1 hurts !
- using windowSize=3 neutral

- neither 1grams nor 3grams bring anything

- smaller suffixes
-> initial test with suffix <= 5 brings a bit for EN, but hurts for DE
-> suffix <= 3 hurts for EN and DE

- generalization of number/date expressions
-> not useful so far (cf. doc/corpus.txt)

- it seems that only dimension around 50 brings something
-> also Turian et al. 2010 only use 50-dims for their word embedding stuff

- Actually, it is unclear, whether GNT features are usable for NER
-> check again paper from Tkachenko & Simanovsky

So, actually, what do I use:

I use this for both EN and DE

-	normalized text: lower case
-	no digit normalization
-	window size: 2
-	no subsampling
-	50 dim indicator words
-	all but only conll-data as unlabeled data
-	shape feature methods from POS tagger for EN and DE
-	all suffix features from training
-	1000 clusterIDs from Marmot
	- NOTE: Marmot cluserID are based on digit-normalized wiki-dumps

Test on DE:
-> best result: 68,41 F1

-> ablation tests (testb):
	cluster and 
	unlabeled features:	-10,42
	cluster features:	-6,87
	unlabeled features: -1,71
	suffix features:	-7,08	
	shape features:		-1,02

Test on EN:
-> best result: 83.29 F1

-> ablation tests (testb):
	cluster and 
	unlabeled features:	-8,99
	cluster features:	-5,58	
	unlabeled features:	-0,56
	suffix features:	-2,7	
	shape features:		-2,47	

Why is GNT-NER so bad compared to GNT-POS ? 
(Basically about -10% F1 for DE and EN, where for GNT-POS it is near state-of-the-art)

HIERIX:

extend training with testa and check how good results are on testb
-> indicates whether extending training would be helpful:

For English:
cat eng-train.conll eng-testa.conll > eng-traintesta.conll
cat eng-train-sents.txt eng-testa-sents.txt > eng-traintesta-sents.txt
For German:
cat deu-train.conll deu-testa.conll > deu-traintesta.conll
cat deu-train-sents.txt deu-testa-sents.txt > deu-traintesta-sents.txt

-> helpful for DE (+2,45% F1) but not for EN


Shape features 
	-> Try out: remove/deactivate suffix features as part of shapw-features
		because they already part of suffix features
	-> DONE: brings a bit for POS and NER, so I deactivate that part of the features
	-> cf. features.WordShapeFeature.createShapeVectorFromWord(String, int)
	
HIERIX: still I am missing about 8%F1 for CONLL

Evaluation with Konvens 2014 DE NER data: DONE

add prefixes as features

Alternative indicator words:
- What if I select indicator words those that occur in training data
	- select the context words from annotated strings 
	and choose the highest ranked words found in all indicator words
	
NER lexicons:
	- although I have word clusters, it might be too small
	- so very large NE lists could make sense
	- or retraining of Marmot cluster using own NER relevant data files





